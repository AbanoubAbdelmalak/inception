// Copyright 2015
// Ubiquitous Knowledge Processing (UKP) Lab and FG Language Technology
// Technische Universit√§t Darmstadt
// 
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
// 
// http://www.apache.org/licenses/LICENSE-2.0
// 
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

[[sect_monitoring_agreement]]
= Agreement

Agreement can be inspected on a per-feature basis and is calculated pair-wise between all 
annotators across all documents.

== Measures

Several agreement measures are supported.

.Supported agreement measures
|====
| Measure | Type | Short description |

| Cohen's kappa
| Coding
| Chance-corrected inter-annotator agreement for two annotators. The measure assumes a different probability distribution for all raters.

| Fleiss' kappa
| Coding
| Generalization of Scott's pi-measure for calculating a chance-corrected inter-rater agreement for multiple raters, which is known as Fleiss' kappa and Carletta's K. The measure assumes the same probability distribution for all raters.

| Krippendorff's alpha (nominal)
| Coding
| Chance-corrected inter-rater agreement for multiple raters for nominal categories (i.e. categories are either equal (distance 0) or unequal (distance 1). The basic idea is to divide the estimated variance of within the items by the estimated total variance.

| Krippendorff's alpha (unitizing)
| Unitizing
| Chance-corrected inter-rater agreement for unitizing studies with multiple raters. As a model for expected disagreement, all possible unitizations for the given continuum and raters are considered. Note that
units coded with the same categories by a single annotator may not overlap with each other.
|====

== Coding vs. Unitizing

Coding measures are based on positions. I.e. two annotations are either at the same position or not.
If they are, they can be compared - otherwise they cannot be compared. This makes coding measures
unsuitable in cases where partital overlap of annotations needs to be considered, e.g. in the case
of named entity annotations where it is common that annotators do not agree on the boundaries of the
entity. In order to calculate the positions, all documents are scanned for annotations and  annotations located at the same positions are collected in configuration sets. To determine if two annotations are at the same position, different approaches are used depending on the layer type. For a span layer, the begin and end offsets are used. For a relation layer, the begin and end offsets of the source and target annotation are used. Chains are currently not supported. 

Unitizing measures basically work by internally concatenating all documents into a single long virtual document and then consider partial overlaps of annotations from different annotations. I.e. there is no averaging over documents. The partial overlap agreement is calculated based on character positions, not on token positions. So if one annotator annotates *the blackboard* and another annotator just *blackboard*, then the partial overlap is comparatively high because *blackboard* is a longish word. Relation and chain layers are presently not supported by the unitizing measures.

Agreement is calculated in two steps:

. *Generation of positions and configuration sets* - all documents are scanned for annotations and 
   annotations located at the same positions are collected in configuration sets. To determine if
   two annotations are at the same position, different approaches are used depending on the layer
   type. For a span layer, the begin and end offsets are used. For a relation layer, the begin and end
   offsets of the source and target annotation are used. Chains are currently not supported. 
. *Calculation of pairwise agreement* - based on the generated configuration sets, agreement is calculated.
  There are two cases where a configuration set may be omitted from the pairwise agreement calculation:
.. one of the users did not make an annotation at the position;
.. one or both of the users did not assign a value to the feature on which agreement is calculated
   at the position.

The lower part of the agreement matrix displays how many configuration sets were used to calculate
agreement and how many were found in total. The upper part of the agreement matrix displays the
pairwise Cohen's kappa scores.

The agreement calculations considers an unset feature (with a `null` value) to be equivalent to a
feature with the value of an empty string. Empty strings are considered valid labels and are not
excluded from agreement calculation.

Annotations for a given position are considered complete when both annotators have made an
annotation. Unless the agreement measure supports `null` values (i.e. missing annotations),
incomplete annotations are implicitly excluded from the agreement calculation. If the agreement
measure does support incomplete annotations, then excluding them or not is the users' choice.

.Possible combinations for agreement
|====
| Feature value annotator 1 | Feature value annotator 2 | Agreement | Complete

| `X`           
| `X`
| yes
| yes

| `X`           
| `Y`
| no
| yes

| *no annotation*           
| `Y`
| no
| no

| *empty*           
| `Y`
| no
| yes

| *empty*           
| *empty*
| yes
| yes

| *null*
| *empty*
| yes
| yes

| *empty*           
| *no annotation*
| no
| no

|====

  
CAUTION: Multiple interpretations in the form of stacked annotations are not supported in the agreement 
      calculation! This also includes relations for which source or targets spans are stacked.

